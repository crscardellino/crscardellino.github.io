<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Resources | Cristian Cardellino]]></title>
  <link href="http://crscardellino.me/blog/categories/resources/atom.xml" rel="self"/>
  <link href="http://crscardellino.me/"/>
  <updated>2016-03-11T10:27:12-03:00</updated>
  <id>http://crscardellino.me/</id>
  <author>
    <name><![CDATA[Cristian Cardellino]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Spanish Billion Words Corpus and Embeddings]]></title>
    <link href="http://crscardellino.me/blog/2016/02/06/spanish-billion-words-corpus-and-embeddings/"/>
    <updated>2016-02-06T16:12:20-03:00</updated>
    <id>http://crscardellino.me/blog/2016/02/06/spanish-billion-words-corpus-and-embeddings</id>
    <content type="html"><![CDATA[<p>So, a year and a half since my last post. Even if I kind of update my page to be
a blog from the root, shame on me.</p>

<p>This blog post however is not something related to what I did in the previous
ones. I promise someday I will continue with my Python to Scala tutorials, but
for now you&rsquo;ll have to settle with this.</p>

<p>Since I am a PhD Student in Natural Language Processing and a native speaker of
the Spanish language, I like to do my research in this language. The problem is
that Spanish, unlike English, doesn&rsquo;t have that many resources.</p>

<p>In the last year I have been working and researching in the fields of deep
learning and word embeddings. The problem with word embeddings, specially with
those generated by neural networks methods like
<a href="https://code.google.com/p/word2vec/">word2vec</a>, is that they require great
amount of unannotated data.</p>

<p>Most of the works I have seen to create Spanish word embeddings use the
Wikipedia, which is a big corpus, but not that big, so I decided to contribute
to the world of word embeddings by first releasing a corpus big enough to train
some decent word embeddings, and then by releasing some embeddings created on my
own.</p>

<p>This is why I am releasing now the <a href="http://crscardellino.me/SBWCE/">Spanish Billion Words Corpus and Embeddings</a>, a resource for the Spanish language
that offers a big corpus (of nearly 1.5 billion words) and a set of word vectors
(or embeddings) trained from this corpus.</p>

<p>Feel free to use it as it is released under a <a href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons BY-SA</a> license.</p>
]]></content>
  </entry>
  
</feed>
